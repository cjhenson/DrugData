{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T13:58:04.683518Z",
     "start_time": "2018-04-29T13:58:04.277006Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T13:58:22.074476Z",
     "start_time": "2018-04-29T13:58:05.169584Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import absolute as nabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load, clean, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:12.366547Z",
     "start_time": "2018-04-29T14:00:12.322053Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def impmat(fname = 'M_processed.mat', writ = True):\n",
    "    ''' import matlab crap, and turn it to pickles (or return panda df)'''\n",
    "    mat = spio.loadmat(fname, squeeze_me=True)\n",
    "    M = mat['M'] \n",
    "    head = ['time','ndc1','ndc2','ndc3',\n",
    "            'Trade_Partner_Name', 'Distribution_Center_State','NDC','Distribution_Center_ID_(IC)',\n",
    "    'Distribution_Center_Zip','Eff_Inv_(EU)','Eff_Inv_(PU)','Qty_Ord_(EU)',\n",
    "            'Qty_Ord_(PU)']\n",
    "    # get rid of ndc 1,2,3 because they're pieces of NCD\n",
    "    # also get rid of purchase units, just use eatable\n",
    "    # also get rid of states and zip code\n",
    "    needed = [0,4,6,7,9,11]\n",
    "    head_adj = [head[i] for i in needed] + [\"year\", \"week\"]\n",
    "    data = pd.DataFrame(M, columns=head)\n",
    "    data[\"time\"] = pd.to_datetime(data[\"time\"], format='%Y%m%d', errors='coerce')\n",
    "    data[\"year\"] = data[\"time\"].dt.year\n",
    "    data[\"week\"] = data[\"time\"].dt.week\n",
    "    #data.drop(\"time\", axis=1)\n",
    "    if writ: # h5 allows your variable to be external\n",
    "        dt = pd.HDFStore(\"drugdata.h5\") # don't need to import/export! warning, though: huge\n",
    "        dt['dat'] = data[head_adj] #\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:13.079620Z",
     "start_time": "2018-04-29T14:00:12.894964Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def test_hd5(p = 0, q = 0):\n",
    "    \"\"\"test data and run answers to intro quiz\n",
    "    p is to print head of dataframe\n",
    "    q prints quiz answers\n",
    "    doesn't return anything\n",
    "    mostly for access examples\"\"\"\n",
    "    dt = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "\n",
    "    header = dt.columns.tolist()\n",
    "    # thanks @brock\n",
    "    def q1(df):\n",
    "        return(df.Trade_Partner_Name.unique())\n",
    "    \n",
    "    def q2(df):\n",
    "        q2 = df.groupby('Trade_Partner_Name')['Distribution_Center_ID_(IC)'].nunique()\n",
    "        q2max = q2.max()\n",
    "        return(q2[q2 == q2max])\n",
    "    \n",
    "    def q3(df):\n",
    "        q3df = df.loc[df[\"time\"].dt.year == 2011] # can also use dt.month\n",
    "        q3TotalSales = q3df.groupby('NDC')['Qty_Ord_(EU)'].sum()\n",
    "        #print(q3TotalSales)\n",
    "        q3sorted = q3TotalSales.sort_values(ascending = False).head()\n",
    "        return(q3sorted)\n",
    "    \n",
    "    def q4(df):\n",
    "        q4 = df['NDC'].value_counts()\n",
    "        NDCLessThan60 = q4[q4 < 60]\n",
    "        if (NDCLessThan60.size == 0):\n",
    "            return(None)\n",
    "        else:\n",
    "            return(NDCLessThan60.size)\n",
    "        \n",
    "    def q5(df):\n",
    "        q5 =  df.groupby('NDC')['Qty_Ord_(EU)'].std()\n",
    "        q5max = q5.max()\n",
    "        NDCHighestVariance = q5[q5 == q5max]\n",
    "        return(NDCHighestVariance)\n",
    "    \n",
    "    def q6(df):\n",
    "        q6 = df.groupby('NDC')['Qty_Ord_(EU)'].nunique()\n",
    "        q6ZeroDemand = q6[q6 == 0]\n",
    "        if (q6ZeroDemand.size == 0):\n",
    "            return(None)\n",
    "        else:\n",
    "            return(q6ZeroDemand.size)\n",
    "    \n",
    "    if p:\n",
    "        for col in header:\n",
    "            print(dt[col].head())\n",
    "    if q:\n",
    "        answers = [q1(dt), q2(dt), q3(dt), q4(dt), q5(dt), q6(dt)]\n",
    "        for i, ans in enumerate(answers):\n",
    "            try:\n",
    "                print('Question %d'%(i+1),  ans)\n",
    "            except:\n",
    "                print('Question %d'%(i+1) + str(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:29.847793Z",
     "start_time": "2018-04-29T14:00:29.843693Z"
    }
   },
   "outputs": [],
   "source": [
    "#impmat(); # uncomment if never built h5 file\n",
    "#test_hd5(q=1) # add p=1 or q = 1 to print stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:27.471727Z",
     "start_time": "2018-04-29T14:00:27.453337Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def rem_neg_vals():\n",
    "    ''' if you've just imported from the mat file,\n",
    "    you need to run this to change the neg vals to 0 '''\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    # set negative values to 0\n",
    "    df.loc[df['Eff_Inv_(EU)'] < 0,'Eff_Inv_(EU)'] = 0\n",
    "    df.loc[df['Qty_Ord_(EU)'] < 0,'Qty_Ord_(EU)'] = 0\n",
    "    df.loc[df['Eff_Inv_(EU)'].isnull(), 'Eff_Inv_(EU)'] = 0\n",
    "    df.loc[df['Qty_Ord_(EU)'].isnull(), 'Qty_Ord_(EU)'] = 0\n",
    "    #print(df.head())\n",
    "    return(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:31.967667Z",
     "start_time": "2018-04-29T14:00:31.311306Z"
    }
   },
   "outputs": [],
   "source": [
    "rem_neg_vals();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Early queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:33.564877Z",
     "start_time": "2018-04-29T14:00:33.545367Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def top_selling(thr, p = 0):\n",
    "    ''' in: minimum contributing percentage threshold\n",
    "        if p, prints number and % of drugs above thr\n",
    "        out: IDs of drugs above thr'''\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    ind_total = df.groupby('NDC')['Qty_Ord_(EU)'].sum()\n",
    "    sortsales = ind_total.sort_values(ascending = False)\n",
    "    #print(sortsales)\n",
    "    total = sum(ind_total.values)\n",
    "    perc_total = 100 * sortsales / total\n",
    "    clipped_above_total = perc_total[perc_total > thr]\n",
    "    if p:\n",
    "        print(len(clipped_above_total), sum(clipped_above_total.values))\n",
    "    return(clipped_above_total.axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T17:11:32.432876Z",
     "start_time": "2018-04-07T17:11:31.932966Z"
    }
   },
   "outputs": [],
   "source": [
    "top_selling(1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:39.891487Z",
     "start_time": "2018-04-29T14:00:39.846595Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def weeks(p = 0):\n",
    "    ''' gives us a list of the weeks as a datetime Series '''\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    ## 2008, at least, does indeed have 52 weeks\n",
    "    ## '07 has 27, '17 has 34\n",
    "    for i in range(2007, 2018):\n",
    "        y2k = df.loc[df.time.dt.year == (i)]\n",
    "        wy2k = y2k.groupby(\"time\").nunique()\n",
    "        ly = len(wy2k[\"time\"])\n",
    "        if p:\n",
    "            print(i, ly)          \n",
    "    #print(df.time.nunique()) # 530 unique dates\n",
    "    return(pd.to_datetime(df.time.unique()).sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T17:11:39.667370Z",
     "start_time": "2018-04-07T17:11:33.095983Z"
    }
   },
   "outputs": [],
   "source": [
    "weeks();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:42.080654Z",
     "start_time": "2018-04-29T14:00:42.064138Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sales_exist():\n",
    "    ''' want to check that every week has sales '''\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    useless = []\n",
    "    for wk in weeks():\n",
    "        sales = df.loc[df[\"time\"] == wk]['Qty_Ord_(EU)'].sum()\n",
    "        if sales == 0:\n",
    "            print(wk)\n",
    "            useless.append(wk)\n",
    "    if not useless:\n",
    "       print(\"sold something every week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T17:11:52.375790Z",
     "start_time": "2018-04-07T17:11:39.685875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sold something every week\n"
     ]
    }
   ],
   "source": [
    "sales_exist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:45.274482Z",
     "start_time": "2018-04-29T14:00:45.261620Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def helper():\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    \n",
    "    print(df.columns.values)\n",
    "\n",
    "    print(df[\"week\"])\n",
    "#helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - The meaty bits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:49.799408Z",
     "start_time": "2018-04-29T14:00:49.783764Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def sales(Year):\n",
    "    \"\"\"in: range of dates want studied \n",
    "    want to return the list of sales per date\n",
    "    todo: break up by location if we want\"\"\"\n",
    "    df = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    \n",
    "    sel_drugs = [i for i in top_selling(1.5)[0]] # list of drug ids\n",
    "    dates = df.loc[df.year == Year] # choose only given year\n",
    "    # gives DF of drugs by week; can change to \n",
    "    # ['NDC', 'time', DISTRO_id] if we want later\n",
    "    window = dates.groupby(['NDC', 'year', 'week'])['Qty_Ord_(EU)'].sum()\n",
    "    filt_window = window.loc[sel_drugs] # only want top drugs\n",
    "    return(filt_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T18:50:01.439165Z",
     "start_time": "2018-04-11T18:50:00.697994Z"
    }
   },
   "outputs": [],
   "source": [
    "sales(2008);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:52.398214Z",
     "start_time": "2018-04-29T14:00:52.389317Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def smape(f, d):\n",
    "    ''' symmetric mean absolute percentage error\n",
    "    in: vectors f = y_hat, d = y \n",
    "    out: the smape, yo '''\n",
    "    n = len(f)\n",
    "    num = np.sum(nabs(f - d))\n",
    "    denom = np.sum(nabs(f) + nabs(d))\n",
    "    return((1/n) * num/denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:00:53.322242Z",
     "start_time": "2018-04-29T14:00:53.309332Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def frame_gen(year):\n",
    "    ''' in: the year starting the frame\n",
    "        out: 2 dfs, 3 years for training and 4th for test'''\n",
    "    window = []\n",
    "    for i in range(3):\n",
    "        window.append(sales(year + i))\n",
    "    # make a table of 3 years\n",
    "    window = pd.concat(window)\n",
    "    test_frame = sales(year + 4)\n",
    "    return(window, test_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:01:03.068465Z",
     "start_time": "2018-04-29T14:01:00.629504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:07:21.471040Z",
     "start_time": "2018-04-29T14:07:21.441183Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def svr_stuff(tst, trn):\n",
    "    ''' input: testing and training data \n",
    "        out: support vector regression score(?) '''\n",
    "    \n",
    "    from sklearn_pandas import DataFrameMapper, cross_val_score\n",
    "    \n",
    "    \n",
    "  #  mapper = DataFrameMapper([('week', le)\n",
    "                             \n",
    "   #                          ])\n",
    "    le = pp.LabelEncoder()\n",
    "    #lb = pp.LabelBinerizer()\n",
    "    y = le.fit_transform(y)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "    print(len(x_train))\n",
    "    \n",
    "    with open(\"out.txt\", \"w\") as f:\n",
    "        for i in range(-2, 2):\n",
    "            svm = SVR(kernel=\"poly\", C=10**i, random_state=0)\n",
    "            svm.fit(x_train, y_train)\n",
    "            f.write(\"c = \" + str(10**i) + \", accuracy = \" + \n",
    "            str(round(svm.score(x_test, y_test), 3)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T22:30:59.151392Z",
     "start_time": "2018-04-08T22:30:59.122833Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def ARIMA(tst, trn):\n",
    "    ''' Alright, Jack, do your thing. \n",
    "    I don't wanna touch this one. '''\n",
    "    from statsmodel.tsa.arima_model import ARIMA\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T20:24:43.173809Z",
     "start_time": "2018-04-06T20:24:43.106595Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def NN_stuff(tst, trn):\n",
    "    ''' All you, Collin '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T20:25:34.070391Z",
     "start_time": "2018-04-06T20:25:33.991217Z"
    }
   },
   "outputs": [],
   "source": [
    "def lasso_stuff(tst, trn):\n",
    "    ''' if we don't like those, we can try this too '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T14:07:32.773718Z",
     "start_time": "2018-04-29T14:07:32.770120Z"
    }
   },
   "outputs": [],
   "source": [
    "### need to have it such that the earlier data is a predictor for the next data\n",
    "### currently it's NOTHING WITH NOTHING VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T18:55:28.557037Z",
     "start_time": "2018-04-11T18:55:28.435301Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    dt = pd.HDFStore(\"drugdata.h5\")[\"dat\"]\n",
    "    spread = [i for i in range(2007, 2015)] # you'll see\n",
    "    for year in spread: # sliding window for analysis\n",
    "        [tst, trn] = frame_gen(year)\n",
    "        print(frame_gen(year))\n",
    "        #seth = svr_stuff(tst, trn)\n",
    "        return()\n",
    "        collin = nn_stuff(tst, trn)\n",
    "        jack = ARIMA(tst, trn)\n",
    "        maybe = lasso_stuff(tst, trn)\n",
    "        which = [seth, collin, jack, maybe]\n",
    "        who = [\"seth\", \"collin\", \"jack\", \"maybe\"]\n",
    "        best_for_year = who[which.index(min(which))]\n",
    "        print(\"The best for\", year, \"is\", best_for_year)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-11T18:55:37.199634Z",
     "start_time": "2018-04-11T18:55:30.871331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NDC    year  week\n",
      "4.0    2007  26      3.360000e+04\n",
      "             27      3.360000e+04\n",
      "             28      0.000000e+00\n",
      "             29      0.000000e+00\n",
      "             30      0.000000e+00\n",
      "             31      0.000000e+00\n",
      "             32      0.000000e+00\n",
      "             33      0.000000e+00\n",
      "             34      3.120000e+04\n",
      "             35      0.000000e+00\n",
      "             36      3.000000e+04\n",
      "             37      0.000000e+00\n",
      "             38      0.000000e+00\n",
      "             39      3.028333e+04\n",
      "             40      3.028333e+04\n",
      "             41      3.028333e+04\n",
      "             42      3.028333e+04\n",
      "             43      7.496667e+04\n",
      "             44      9.085000e+04\n",
      "             45      1.845840e+07\n",
      "             46      9.993600e+06\n",
      "             47      1.114560e+07\n",
      "             48      1.678800e+07\n",
      "             49      1.439280e+07\n",
      "             50      2.508610e+07\n",
      "             51      2.131080e+07\n",
      "             52      1.900960e+07\n",
      "7.0    2007  26      1.680000e+03\n",
      "             27      1.680000e+03\n",
      "             28      0.000000e+00\n",
      "                         ...     \n",
      "154.0  2009  23      3.250603e+06\n",
      "             24      3.740623e+06\n",
      "             25      4.909483e+06\n",
      "             26      3.339043e+06\n",
      "             27      3.069043e+06\n",
      "             28      3.207960e+06\n",
      "             29      3.621027e+06\n",
      "             30      3.062743e+06\n",
      "             31      3.532723e+06\n",
      "             32      3.391183e+06\n",
      "             33      3.557083e+06\n",
      "             34      2.988927e+06\n",
      "             35      3.636987e+06\n",
      "             36      3.608323e+06\n",
      "             37      3.229380e+06\n",
      "             38      3.398820e+06\n",
      "             39      3.626280e+06\n",
      "             40      3.404940e+06\n",
      "             41      2.639383e+06\n",
      "             42      3.292243e+06\n",
      "             43      2.609503e+06\n",
      "             44      2.772703e+06\n",
      "             45      2.612323e+06\n",
      "             46      2.477623e+06\n",
      "             47      2.457463e+06\n",
      "             48      1.979563e+06\n",
      "             49      2.988763e+06\n",
      "             50      2.580583e+06\n",
      "             51      3.192943e+06\n",
      "             52      2.266843e+06\n",
      "Name: Qty_Ord_(EU), Length: 1405, dtype: float64, NDC    year  week\n",
      "4.0    2011  1       3.157500e+05\n",
      "             2       1.587500e+05\n",
      "             3       4.991500e+05\n",
      "             4       4.105500e+05\n",
      "             5       3.632500e+05\n",
      "             6       4.052500e+05\n",
      "             7       3.374667e+05\n",
      "             8       5.862667e+05\n",
      "             9       2.508667e+05\n",
      "             10      3.933667e+05\n",
      "             11      3.462000e+05\n",
      "             12      3.640000e+05\n",
      "             13      1.215783e+06\n",
      "             14      9.431667e+05\n",
      "             15      8.163667e+05\n",
      "             16      8.627500e+05\n",
      "             17      8.685667e+05\n",
      "             18      1.266667e+05\n",
      "             19      1.300667e+05\n",
      "             20      1.574667e+05\n",
      "             21      1.293667e+05\n",
      "             22      1.378667e+05\n",
      "             23      1.452667e+05\n",
      "             24      1.490667e+05\n",
      "             25      2.554833e+05\n",
      "             26      1.224833e+05\n",
      "             27      1.804833e+05\n",
      "             28      1.491833e+05\n",
      "             29      1.659833e+05\n",
      "             30      1.706833e+05\n",
      "                         ...     \n",
      "157.0  2011  23      5.261613e+05\n",
      "             24      6.117813e+05\n",
      "             25      7.374600e+05\n",
      "             26      4.435800e+05\n",
      "             27      4.521000e+05\n",
      "             28      4.576200e+05\n",
      "             29      5.697600e+05\n",
      "             30      7.612200e+05\n",
      "             31      7.008600e+05\n",
      "             32      8.787600e+05\n",
      "             33      6.535800e+05\n",
      "             34      8.653200e+05\n",
      "             35      4.992000e+05\n",
      "             36      6.213600e+05\n",
      "             37      6.478200e+05\n",
      "             38      6.124800e+05\n",
      "             39      9.415800e+05\n",
      "             40      7.381800e+05\n",
      "             41      7.662000e+05\n",
      "             42      7.396200e+05\n",
      "             43      7.154400e+05\n",
      "             44      8.322000e+05\n",
      "             45      1.261980e+06\n",
      "             46      9.843000e+05\n",
      "             47      7.412400e+05\n",
      "             48      7.149000e+05\n",
      "             49      8.093400e+05\n",
      "             50      9.406800e+05\n",
      "             51      1.496580e+06\n",
      "             52      9.498000e+04\n",
      "Name: Qty_Ord_(EU), Length: 712, dtype: float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
